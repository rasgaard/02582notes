# Week 12

Literature: WireOverview.pdf
Subjects: Multiway Models

# Introduction

## Tensors

Multiway arrays are tensors. 

- An order-1 tensor is a vector, $\mathbf x$ where you access individual elements $x_i$
- An order-2 tensor is a matrix, $\mathbf X$ where you access individual elements $x_{i,j}$
- An order-3 tensor is a 3-way array, $\mathbf{\mathcal{X}}$ where you access individual elements $x_{i,j,k}$
- This goes on...

As an example, you could measure temperature in degrees Celsius. A single observation would be an order-0 tensor, but you could imagine that you collect temperature over time and store that in a order-1 tensor. We can introduce another dimension in that we collect observations in temperature on a specific date each month. This would then be stored in a order-2 tensor with time going vertically and month going horizontally. This can be further extended by having year in the third dimension of an order-3 tensor.

These multiway structures are widely ignored in many fields of science and engineering.

## What to gain?

An advantage, especially for the Parafac model, is that we can obtain unique solutions. This is without imposing additional constraints such as independence of components or orthogonality.

We can identify components even when facing very poor signal to noise ratios (SNR). This is where other unsupervised methods such as PCA fail a bit to find underlying structure in the data.

It also handles data sets with large amounts of missing data.

The models explicitly take the multi-way structure of the data into account.

Interpretability is also increased.

# Notations and Operations

Tensors are denoted using calligraphic letters

$$
\mathcal X \in \mathbb R^{I_1 \times I_2 \times \dots \times L_N}
$$

- $N$ denotes the order of the tensor
- $I_n$ denotes the dimensionality of the $n-$th mode (number of variables in that mode/dimension)

![Untitled](Week%2012%2048961d12464d40d2809c2607c94b0aea/Untitled.png)

### Frobenius Norm of a tensor

We take the square root of the sum of the square of all individual elements

$$
||\mathcal A||_F = \sqrt{\sum_i \sum_j \sum_k a_{ijk}^2} 
$$

### Matricising tensors

This is the process of “unfolding” a tensor to a matrix with respect to a given mode. Think reshaping tensors to lower dimensions.

### N-mode multiplications

The $n-$mode multiplication of an order $N$ tensor $\mathcal X^{I_1\times I_2 \times \dots\times I_N}$ with a matrix $M^{J\times I_n}$ is given by:

$$
\mathcal X \times_n M = \mathcal Z ^{I_1 \times \dots \times I_{n-1} \times J \times I_{n+1} \times \dots \times I_N}
$$

This operation is defined such that if the tensor $\mathcal X$ is unfolded along the $n$th mode and becomes $X_{(n)}$

$$
[\mathcal X \times_n M]_{(n)}= MX_{(n)}
$$

# Tucker Model

This will look at TUCKER3 as decomposition of a 3rd order tensor $\mathcal X^{I\times J\times K}$.

This decomposition will take in tensor $\mathcal X$ and decompose it into a core tensor $\mathcal G$ and loading matrices $\mathbf A, \mathbf B, \mathbf C$.

A way to understand the Tucker model is a sum of outer vector products which are scaled by the core tensor elements.

$$
\mathcal X = \sum_{p=1}^P \sum_{p=q}^Q \sum_{r=1}^R g_{pqr} a_p \circ b_q \circ c_r = \mathcal G\times_1 \mathbf A \times_2 \mathbf B \times_3 \mathbf C
$$

The TUCKER3 model could also be formulated in the scalar form

$$
x_{ijk} \approx \sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R g_{pqr} a_{ip} b_{jq} c_{kr}
$$

Another formulation is the matrix form, where we calculate each unfolded tensor for a given mode.

$$
\begin{align*}
\mathbf X_{(1)} \approx~& \mathbf{AG}_{(1)} (\mathbf{C} \otimes \mathbf B)^T \\
\mathbf X_{(2)} \approx~& \mathbf{BG}_{(1)} (\mathbf{C} \otimes \mathbf A)^T \\
\mathbf X_{(3)} \approx~& \mathbf{CG}_{(1)} (\mathbf{B} \otimes \mathbf A)^T \\
\end{align*}
$$

where $\mathbf A \otimes \mathbf B$ is the Kronecker product where it yields all possible outer vector products between columns in $\mathbf A$ and $\mathbf B$.

To estimate the TUCKER3 model we need an objective which is to minimise the construction error.

$$
\min_\mathcal{X} |\mathcal X - \mathcal{\hat X}|_F
$$

This is solved using Alternating Least Squares (ALS)

# Parallel Factor Analysis (PARAFAC)

Sometimes also called canonical decomposition (CP).

We can understand PARAFAC as decomposing a tensor into a sum of component rank-1 tensors.

This is also formulated in multiple different ways, but has no core tensor.

It often makes sense to rerun the ALS algorithm as there are many local minima that can result in non-optimal convergence. 

# Tucker Vs. PARAFAC

PARAFAC is a special case of the TUCKER model. It is more restricted as it is only allowed to have 1s in the core tensor’s superdiagonal and all other elements have to be 0. This implies that only the $n$th loading of a given loading matrix can interact with the corresponding $n$th loading of another loading matrix. 

TUCKER is in general very good at data compression tasks, as well as when the data has different ranks and modes.

PARAFAC is good for situations where additive (physical) profiles are to be deconvoluted/resolved from each other.

PARAFAC has a property of uniqueness. This is that given the property of the super diagonal core tensor, the PARAFAC models yields unique solutions. The Tucker model is not unique and as such multiplying an invertible matrix $Q^{L\times L}$ gives an equivalent representation.

Another way to put this is that the Tucker model has rotational freedom.