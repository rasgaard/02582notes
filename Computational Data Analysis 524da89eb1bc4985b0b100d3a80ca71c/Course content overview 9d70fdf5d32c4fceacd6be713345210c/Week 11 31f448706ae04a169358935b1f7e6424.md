# Week 11

Exercises: https://drive.google.com/file/d/1iJKzxKqyvOlX_yIWztWS3QsUhdrDE1fn/view?usp=sharing
Literature: ESL Chapters: 11.1-11.5, 14.4
Slides: https://drive.google.com/file/d/1Q6g5qR-mbMYdXeWUJWyLwjxHuSUz9Pfx/view?usp=sharing
Subjects: Artificial Neural Networks and Self Organizing Maps

# Introduction

ANNs are used for non-linear regression and classification.

With regular machine learning you have some input for which you have a data scientist to do some feature extraction and then feed it into an algorithm that provides some output.

For deep learning the neural network does the feature extraction as well as provide the output.

# The basics of neural networks

Researchers took inspiration for the ANN from the brain as it is built on a network of neurons $\approx 10^{11}$ and a typical neuron has $\approx 10^3$ inputs (dendrites) and one output (axon).

We would like to have a flexible and general function that maps input to outputs for all kinds of problems. It is built around simple functions taking many inputs and giving one output. These functions can be nested into a network such that the output from one function is the input to another function. For simplicity we put these functions in layers.

The artificial neuron is the simplest building block that we stack in layers to create networks. It takes a weighted sum of the inputs and gives a non-linear transform as output. This activation function was originally chosen to be the sigmoid function,

$$
\sigma(v) \frac{1}{1 + \exp(-v)}
$$

We know the sigmoid function from logistic regression.

# Fitting ANNs

We have some parameters called weights that are to be fitted to the training data by minimising some loss function $\mathcal L(W)$.

For regression the loss term is ,

$$
\mathcal L_{\text{regr}} (W) \sum_{k=1}^K \sum_{i=1}^N (y_{ik} - f_k(x_i))^2
$$

For classification the loss term is,

$$
\mathcal L_{\text{cls}} (W) = - \sum_{k=1}^K \sum_{i=1}^N y_{ik} \log f_k(x_i)
$$

which is the cross-entropy loss and the output will be a logistic regression in the hidden layer output.

The new weights are then updated in terms of the old weights through gradient descent and back-prop.

$$
w^{\text{new}} \leftarrow w^{\text{old}} - \eta \frac{\partial J(w, b)}{\partial w}
$$

Some design variables (hyperparameters) are hidden layers and hidden units. These adjust the complexity of the model.

It is often a good idea to start out with a model that is too complex and then regularise away complexity.

The error-function is non-convex and possesses many local minima. This motivates trying out different starting values and then choosing the best model. Also using the average prediction from several networks as the final prediction. Finally the use of bagging.

# Overfitting

The ANNs are super flexible they are known to overfit and we want to avoid this. Techniques such as cross-validation, early stopping, regularisation, dropout and data augmentation all help with preventing overfitting.

# Autoencoders

This is a neural network architecture which has an encoder and a decoder. The encoder encodes the input into a latent space embedding and the decoder decodes that embedding into a reconstruction of the original data.

# Self Organising Maps

This is an unsupervised clustering method which uses a projection of data to a low dimensional space.

Observations are grouped into clusters much like K-means clustering, but the difference is that clusters have neighbours in a one or two dimensional grid and that neighbours should lay close to each other also in feature space. This creates a mapping of data down to one or two dimensions.

Useful for visualisation (sort of like t-SNE, UMAP, etc.).