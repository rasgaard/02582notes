# Week 1

Exercises: https://drive.google.com/file/d/1mJUhT0PJQfcVVGihEiw5UNbITkciIfYK/view
Literature: ESL Chapters: 1, 2.1-2.3, 2.9, 3.1, 3.2, 3.4.1, 4.1, 13.3
Slides: https://drive.google.com/file/d/19u8g8sAYHwSC8azVrx-GteZkI0WtK37C/view?usp=sharing
Subjects: Introduction, EPE, bias, variance, collinearity (OLS, Ridge, KNN, K-means)

# Definitions and applications

## Definitions

> **Machine learning**
”Field of study that gives computers the ability to learn without being explicitly programmed”
- Arthur Samuel (1959)
> 

Machine learning vs data mining

**Machine learning** focuses on prediction based on *known* properties learned from the trained data

**Data mining** focuses on the discovery of (previously) unknown properties in the data

Professor Michael Jordan, UC Berkeley states that we haven’t reached the full potential of AI and ML yet which will be obtained when AI, Intelligent infrastructure and intelligent augmentation plays well together.

## Applications

Classification of bicycle accidents using SVM

Classification of faces using AdaBoost

A load of applications can be found everywhere. 

# Terminology

Agile development ties into how you should carry out your experiments in machine learning. Ref. Machine Learning Yearning.

![Untitled](Week%201%20636c357c0d09426b8fe5d9be2525b534/Untitled.png)

We focus on two types of problems

- Supervised learning
    - We have some input data and a response to that data.
    - Relies on valid label assignment (data quality) and a useful response
- Unsupervised learning
    - No label/response given, so the algorithm learns directly from the data

| Machine learning | Statistics | Pattern recognition |
| --- | --- | --- |
| X inputs | X predictors | X features |
| Y outputs | Y responses | Y responses |

An input matrix **X** of size (*n x p*) will have *n* samples/rows and *p* variables/columns.

An output matrix **y** of size *(n x 1)* will have 1 prediction for each of the *n* samples/rows

# Ordinary Least Squares

Model: $y = X\beta + e$

To find the optimal $\beta$ we want to minimise

$\| y - X\beta \| _2^2 = \sum_{i=1}^n(y_i - X_i\beta)^2$

This was shown analytically by Gauss

$$
||y - X\beta||_2^2 = \sum_{i=1}^n(y_i - X_i\beta)^2 \\
\beta_{OLS} = \text{argmin}_\beta ||y - X\beta||_2^2 \\
\frac{\partial}{\partial \beta}(y-X\beta)^T(y-X\beta)=0\\
-2X^T(y-X\beta)=0\\
\beta_{OLS}=(X^TX)^{-1}X^Ty
$$

Regression can be done to obtain a prediction model $\hat{y}(x) = x\hat{\beta}$.

- More variables means **more uncertainty** and **more observations** means less uncertainty
- **Removing variables** can decrease variance but it comes at a price. There will be a **systematic error** (bias) due to missing variables

OLS is the **best linear unbiased estimate** (BLUE)

- Unbiased $E(\beta_{OLS}) = \beta$
- Best unbiased: $\text{Var}(\beta_{OLS}) \leq \text{Var}(\beta_{linear})$

# Bias and variance

We can characterise a model in terms of its **bias** and **variance**.

Methods in this course often aim to **lower variance**  - at the price of **increasing bias.**

## Bias

**What is it?**

- The bias is the difference between an expected value and the true value.

**Bias of what?**

- Could be of $\beta$ the model parameters
- Could also be of the predictions $\hat{y}$

**Unbiased?**

- Repeat experiments and take average of $\beta$ or $\hat{y}$
- Will be equal to the true value or unbiased

## Variance

We might be right on average (unbiased) but we only do one experiment. How far are we from the true value? This depends on the variance.

- **High variance,** we might end up far from the true value.
- **Low variance,** we get the same result every time, how far it is from the true value depends on the bias.

![Untitled](Week%201%20636c357c0d09426b8fe5d9be2525b534/Untitled%201.png)

# Expected Prediction Error (EPE)

What happens to OLS when there are more variables than samples, $p > n$?

In the OLS solution $\beta_{OLS} = (X^TX)^{-1}X^Ty$ we are no longer able to invert $X^TX$ as it becomes singular when it has more variables than observations.

The EPE is given by 

$$
EPE(x_0) = \lim_{k\rightarrow\infty} \frac1k \sum_{i=0}^k \|\|y_i(x_0) - \hat{f}(x_0; D_i)\|\|^2
$$

which is the expected squared prediction error if we repeat the experiment many (infinite) times. The training data $D_i$ is selected at random. We’re trying to reach the lowest EPE possible.

The EPE consists of 3 error-components:

- Irreducible error: $\sigma_e^2 = E_y(y(x_0) - f(x_0))^2$ which is the squared expected difference between the predicted and actual value.
- $\text{Bias}^2(\hat{f}(x_0; D)) = (E_D(\hat{f}(x_0;D)) - f(x_0))^2$ which is given by the squared difference between the expected prediction and the true value.
- $\text{Variance}(\hat{f}(x_0;D)) = E_D(\hat{f}(x_0;D) - E_D(\hat{f}(x_0;D)))^2$ which is the expected difference between the predicted value and the expected predicted value. This measures how far we are off the average in our prediction.

![Untitled](Week%201%20636c357c0d09426b8fe5d9be2525b534/Untitled%202.png)

In OLS there is no bias and a lot of variance. Through introducing some bias we can lower the variance.

![Untitled](Week%201%20636c357c0d09426b8fe5d9be2525b534/Untitled%203.png)

# Ridge regression

We wish to lower the variance of OLS $\hat{y} = X\beta$, and lowering the size of $\beta$ will lower the variance of $\hat{y}.$

We introduce a **shrinkage**-parameter, $\lambda$, to penalise values in $\beta$ that are too large.

$\hat{\beta}_{ridge} = \text{argmin}_\beta\|\|y - X\beta\|\|^2 + \lambda \|\|\beta\|\|^2$

This also has a closed-form solution

$\beta_{ridge} = (X^TX + \lambda I)^{-1}X^Ty$

We are adding a small number to the diagonal of the matrix to invert. This stabilises the inverse numerically and ridge regression solutions are available even when $p > n.$

# Classification

Linear classifiers use a straight line / hyperplane in higher dimensions to separate classes.

## Fischer’s Linear Discriminant Analysis

Find a linear combination $Z = a^TX$ such that the between-class variance is maximised to the within-class variance.

![Untitled](Week%201%20636c357c0d09426b8fe5d9be2525b534/Untitled%204.png)

The above image represents finding the maximum between-class variance and the below represents finding the minimum within-class variance.

## K-Nearest Neighbour

Classify observations according to the **majority class** of the *K nearest neighbours.*

- Distance of proximity e.g. euclidean distance.
- It is general practise to standardise each variable to mean zero and variance to 1 as it makes sense to see them in the same scale.
- *K* is a positive integer of your choice. Small values give low bias, large values give low variance.

Even though it’s easiest to see this as a classification method it can also be used for regression. It would take the average response of the *K* nearest neighbours.

# Summary points

- Purpose of different types of methods
    - Supervised
    - Unsupervised
- Regression (pros, cons and when to use)
    - Ordinary Least Squares (OLS)
        - Unbiased, high variance, $n > p$
    - Ridge regression
        - Biased, lower variance, $p > n$
    - KNN regression
- Classification (pros, cons and when to use)
    - Fischer LDA
        - Linear
    - KNN classification
        - Flexible
- Bias-variance trade-off and EPE
