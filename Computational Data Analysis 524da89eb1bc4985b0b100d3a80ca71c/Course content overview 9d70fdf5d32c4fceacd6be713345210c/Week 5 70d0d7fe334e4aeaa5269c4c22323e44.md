# Week 5

Exercises: https://drive.google.com/file/d/1qnRhEsiHxADgH10-jng0q7usyk0wHzCv/view?usp=sharing
Literature: ESL Chapters: 9.2, 8.7
Slides: https://drive.google.com/file/d/1Fj6miQuJPWWwqzLeBHespfQEe-LvjSbd/view?usp=sharing
Subjects: Tree based methods [CART and bagging]

# Regression trees

- Choose a number of knots $k$
- Try all possible possible positions for each knot $k$
    - Infinite number of combinations - not feasible
    - Instead try those knots that split the data the best
    
    ![First knot places that splits the data the best](Week%205%2070d0d7fe334e4aeaa5269c4c22323e44/Untitled.png)
    
    First knot places that splits the data the best
    
    ![After three knots are placed](Week%205%2070d0d7fe334e4aeaa5269c4c22323e44/Untitled%201.png)
    
    After three knots are placed
    

This begs the question: **What is a good split? And how do we know how many splits we need to perform in order to fit the data well?**

## Deciding where and how much to split

The terminal nodes each represent an interval of the input variable(s). We choose to represent the outcome in this interval by a **constant** function. As for most regression problems we say that a constant function is good if it has **low residual sum of squares** when compared to training outcome data.

![Untitled](Week%205%2070d0d7fe334e4aeaa5269c4c22323e44/Untitled%202.png)

For the interval $I$ we fit the constant function (the dotted line) and compute the residual sum of squares (RSS) for that interval. 

$RSS_I = \sum_{i\in I}(y_i-\hat{y}_i)^2$

![Untitled](Week%205%2070d0d7fe334e4aeaa5269c4c22323e44/Untitled%203.png)

For an interval you would split the middle and it won’t change as the average of the points in the interval.

![Untitled](Week%205%2070d0d7fe334e4aeaa5269c4c22323e44/Untitled%204.png)

In an interval with $n_i$ observations we have $n_i$ possible splits. The approach is basically to try them all and compare the RSS. Then finally choose the one that results in the lowest total RSS on the interval.

## How large do we grow the tree?

Stop splitting when a node contains too few observations, i.e. 10 or less (`min_samples_split` in sklearn).

The procedure is as follows:

- First interval (node) is the entire range of $X$.
- For each variable
    - For each split position calculate $RSS = RSS_{left} + RSS_{right}$
    - Remember position with the lowest $RSS$
- Split the variable with the lowest $RSS$ at the corresponding position.
- This produces a left and right sub-interval (child-nodes)
    - Split each of these into child nodes as above.
    - Keep splitting nodes until a node contains too few observations.
- Assign a constant function to terminal nodes, the average of the observations.

Deciding when to stop splitting might be hard as you might miss out on good splits which would in turn lead to overfitting. This is solved by **pruning.** The idea is to grow a large tree and cut away branches that are not meaningful.

## Pruning

We are interested in pruning the non-terminal nodes whose sub-tree gives the smallest **per node** reduction in $RSS.$ I.e. we divide the reduction by the number of terminal nodes minus one.

So now that we have a method for removing sub-trees that contribute the least in terms of per-node reduction to the $RSS$, we would like to know when to stop pruning. Luckily this is easily done using an independent test set or cross-validation.

## Bias and variance

Full-grown trees with no pruning would result in high variance and low bias as it is clearly overfitting.

With the introduction of pruning we get lower variance and introduce some bias.

The bias and variance depends on the depth of the tree. A tree with no pruning results in high variance and little to no bias, while pruning results in lowering the variance and increasing the bias. 

A tree with a single split would be very biased.

# Classification trees

For this type of tree the end nodes are classes and as such we are looking at the majority class for choosing the prediction.

In regression trees we used $RSS$ as a metric for node impurity i.e. where to split the data. In classification trees we look at

- Misclassification rate
- Gini index
- Cross-entropy

which all favour the split that increases purity the most. Typically the predictive performance between the three metrics is not that different. The shape of the tree might be very different.

In a specific node, representing a region $R$ with $N$ observations, let the purity be defined as

$$
\hat{p}_k = \frac1N \sum_{x_i\in R} 1\!\!1 (y_i = k)

$$

Classify observations in the node to class

$$
K = \arg\max_k \hat{p}_k
$$

Measures of impurity within a node,

| Misclassification error | ⁍ |
| --- | --- |
| Gini index | ⁍ |
| Cross-entropy (deviance) | ⁍ |

The node impurity, $Q$ , is weighted with the number of observations in each node, $N$.

The split decision is based on the split that minimises

$$
N_{left} Q_{left} + N_{right}Q_{right}
$$

# Benefits of tree based methods

Interpretability. Often used in medical sciences because it may represent the way doctors reason.

A single tree describes the entire partitioning on the input space.

With $p > 3$ input variables, the partition (also though of as knot positions) are difficult to visualise, but a tree representation is always possible.A large tree might be difficult to interpret anyway.

Trees also handle missing data very well.

# Bagging

*Short for bootstrapped aggregation*

Here we’ll use bootstrap to improve predictions. Bagging averages predictions over a collection of bootstrap samples. Average may be noisy but approximately unbiased models and thereby reduce the variance.

If we make $B$ bootstrap samples of size $N$ we repeat a model fit using the $b$th sample and make the prediction $\hat y_b$. The bagging estimate is given by the average of the $B$ predictions

$$
\hat y_{bagging} = \frac1B \sum_{b=1}^B \hat y_b
$$

Particularly good for high-variance, low-bias methods such as trees.

Regression trees are fitted to bootstrap samples of the training data. The result is the average over all the trees.

For classification trees, a committee of trees each cast a vote for the class, and the majority vote is used as the prediction.