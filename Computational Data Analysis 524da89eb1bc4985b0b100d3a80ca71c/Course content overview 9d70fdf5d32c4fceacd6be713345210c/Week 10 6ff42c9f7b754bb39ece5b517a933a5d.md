# Week 10

Exercises: https://drive.google.com/file/d/1XrFglibYOXm5ZvA2AV0yKuCq_LFd9kTy/view?usp=sharing
Literature: ESL Chapters 14.6, 14.7 and article ‘Sparse coding’
Slides: https://drive.google.com/file/d/1_G02AU_slizakdbD2Ig6sp8CSGQcGeiy/view?usp=sharing
Subjects: Unsupervised decompositions [SC, NMF, AA and ICA]

# Introduction

## Supervised and Unsupervised learning

- Supervised Learning
    
    Here we are given input and output data for a given task for which we wish to predict the output, $y$, for an input $x$, $p(y \vert x)$
    
- Unsupervised learning
    
    Here we are only given input data which causes us to find patterns and characteristics by looking at the data itself.
    

The purpose of unsupervised learning can be summarised as extracting an efficient internal representation of the statistical structure implicit in the inputs.

# Non-negative Matrix Factorisation

They found that decomposing a matrix from a dataset of face-images where NMF would decompose into $X = WH+E, \quad W\geq 0, H\geq 0$ and the decompositions would be parts of the face itself. So the nose, beard, eyes, etc. 

For text information retrieval the when decomposing a Term/Document matrix the $W$ matrix would correspond to the various topics found in the documents. A given document would then be reconstructed in terms of how it used these various terms and to what extend.

A problem with NMF is that the decomposition is not in general unique

$$
X \approx WH = (WQ^{-1})(QH)=WH \rightarrow W\geq0, H\geq0
$$

Looking at the data matrix $X$ which is assumed to be positive (which it is in the case of document term frequencies, pixel intensities, etc.). We could look at the observation space that perfectly captures our observations which would decompose to the same as a larger observation space which also hold our observations. By having a sparse NMF decomposition we can impose regularisation with the L1-norm we can disambiguate between the two observation spaces.

# Archetypal Analysis

This method decomposes a matrix $X$ into $X$ itself as well as $S$ and $H$

$$
\begin{align*}
X = XSH + E \\
|S_d|_1 = 1, \quad|h_j|_1 = 1 \\
S \geq 0, \quad H\geq 0
\end{align*}
$$

This states that the $S$ matrix is non-negative and its columns has to sum to 1. The $H$ matrix is also non-negative and sums to 1 over its columns.

These are convex combinations as the columns in $S$ generates a weighted average for each observation in $X$. The $XS$ matrix creates sort of archetypes for each observations and the $H$ matrix explains how an observation is constructed using these different archetypes.

For the dataset of faces an archetypal analysis would result in finding archetypes with very pronounced features such as thick glasses or, huge smile, large moustache, etc. In other words very distinct characteristics which then would also be able to reconstruct the dataset.

# Independent Component Analysis

A modern approach to the rotational ambiguity within factor analysis.

We can assume $X$ is pre-whitened such that $XX^T=I$.

If this is not the case perform SVD

$[U, \Sigma, V^T] = \text{SVD}(X)$

And perform ICA on $Y = V$

$[\tilde{A}, S] = \text{ICA}(Y)$ then $X = U\Sigma \tilde{A} S = AS$ where $A = U\Sigma \tilde A$

$S$ is assumed to be independent which implies that it is uncorrelated, but that is a weaker condition that statistical independence.

If we say that components of $S$ are uncorrelated then they are orthogonal to each other, $SS^T = I$, but as $YY^T=\tilde A SS^T \tilde A^T = \tilde A \tilde A = I$ is a pre-whitened matrix we get that $\tilde A$ is an orthonormal matrix.

Thus ICA mounts to solving for an orthonormal matrix $\tilde A$ such that $S = \tilde A^TY$.

The central limit theorem states that the more we add the more Gaussian it becomes. We can use Kurtosis to determine the level of Gaussianness. The further away from 0 the Kurtosis is, the less Gaussian.

# Sparse Coding (Dictionary Learning)

In this method we decompose a data matrix $X$ into $A$ and $S$ where we preserve as much information as possible while also preserving simplicity, or sparsity, in $S$ which is controlled by the trade-off parameter $\lambda$.

$$
\arg\min_{A, S} D(X, AS) + \lambda sp(S)
$$

A measure of sparsity would be given by 

$$
sp(s) = |s|_\gamma = \sum_d |s_d|^\gamma
$$

for which the ultimate measure of sparsity is given by $l_0$ norm, where we directly minimise the number of non-zero entries, however this results in an NP-hard optimisation problem. The $l_1$ norm is commonly invoked.

$$
\arg\min_{A, S} \frac12 ||X - AS||^2_F + \lambda |S|_1
$$

Solving $S$ for fixed $A$ correspond to a series of sparse regression problems

$$
\arg \min_{s_j} \frac12 ||x_j - As_j||^2_F + \lambda |s_j|_1
$$

We use the $l_1$ norm as it is the closest convex proxy for the $l_0$ norm.
