# Week 8

Exercises: https://drive.google.com/file/d/1--aENTxyYsWv100v7L-kTyAe6Px8XjQ2/view?usp=sharing
Literature: ESL Chapters: 14.5.1, 14.5.5, 3.5, 3.7
Slides: https://drive.google.com/file/d/1e9KR_9A80KdC0JNHmYmKzxSBgEfrXePh/view?usp=sharing
Subjects: Subspace methods [PCA, SPCA, CCA, PLS]

# PCA

As opposed to previous methods, PCA is exploratory and unsupervised. The data speaks for itself and we don’t have any targets.

We utilise that in high dimensions data is clustered in space, data lie on a low-dimensional manifold and variables are correlated.

The idea of PCA is that it’s a linear transformation of data $S = XL$ such that we are able to go back after transforming. It preserves relations (angles) between variables $S = XL$ subject to $L^TL=I$ which is an orthogonal transformation and $L$ is a rotation matrix.

It rotates such that the projected data $S$ has maximum variance and successively maximises the variance of the principal components.

![Untitled](Week%208%207f5292e5db8d4f59b123b79f0f6f279d/Untitled.png)

Here the data has been centered and rotated such that the principle components lie where the data has maximum variance.

## Scores and Loadings

$$
S = XL
$$

$S$ - **the scores**

- Size is $n \times m$, $m = \min(n,p)$
- Coordinates of data points on new axes
- Columns of $S$ are the **principal components (PC)**
- PCs are uncorrelated - $S^TS$ is diagonal

$L$ - **the loadings**

- Size is $p \times m$, $m = \min(n,p)$
- Columns are known as **the principal axes**
- Rotation matrix $L^TL=I$
- Columns are orthogonal and of unit length

PCA is used to reduce the number of dimensions of the data, or rather keep the dimensions that holds the most variance. In a sense “compressing” the data.

Here is a data matrix with 4 variables where we keep 3 principal components due to multiplying only three loadings onto the data

$$
[s_1 s_2 s_3]_{n\times3} = [x_1x_2x_3x_4]_{n\times4}[l_1l_2l_3]_{p\times3}
$$

To assess the number of components to keep we have previously (in 02450) used the percentage of explained variance as a threshold to see if we have, for example, 95% explained variance with a given number of components. This is apparently not always a good idea. Another approach is to keep eigenvectors with eigenvalues greater than one for the correlation matrix. This is known as the **Kaiser criterion**. We can also create a **skree plot** where we compare the eigenvalues to those obtained from randomised data.

The reason why we don’t necessarily want to look at explained variance is because it is a relative measure and varies from data set to data set. 

An important thing to note is that the backbone of all this is correlation and correlation might be coincidental and not have any associated meaning which will skew interpretation.

# Sparse PCA

The benefits of PCA is that it is the only linear transform with independent loading vectors and uncorrelated scores. The linear transformation that gives the most compact data representation. It is also easy and quick to calculate using SVD no matter how $p$ and $n$ are.

However, for understanding data it is not great. In many analyses we need to understand what is going on in terms of the original variables and not the principle components.

Each principle component is a linear combination of **all** variables.

$s_1 = l_{11}x_1 + l_{21}x_2 + l_{31}x_3 + l_{41}x_4$

For sparse PCA we approximate $s_1$ but drive some coefficients to zero. 

To set the scene let’s consider the spectrum of sparsity where in one corner we have regular PCA with no imposed sparsity and in the other corner we have that the loadings are all equal to 0.

We’ll be looking at three techniques for sparse PCA

**Thresholding**

In the simplest sense just set all loadings below a certain threshold to zero. This method is not desirable if we have some sort of correlation between observations since we essentially go sharply from something to 0.

**Varimax**

This produces approximately sparse loading vectors, so they are not exactly zero, but rather adjusts the loadings such that some are really big and others are really small.

We maximise the variance among the loadings in each principal axis gives approximate sparseness. The amount of sparsity is determined by $k$, the number of retained principal components.

**Estimation using elastic net**

We can express each principal component as a regression problem and optimise with respect to $l$ using scores $s_i$ from PCA.

$$
\arg\min_l ||s_i - Xl||^2
$$

$$
\arg\min_l || s_i Xl||^2 + \lambda||l||^2
$$

# Principal Component Regression

This is a linear regression where the input is the scores $S$ from the PCA (recall that $S = XL$).

We choose a subset of the scores for some $M < p$ and have a standard regression problem in the new variables.

PCR handles $n< p$ by operating on a subset of PCs. PCR performs similar to ridge regression. Equivalent to OLS when $M=p$.

# Partial Least Squares

This is a supervised method with a latent variable structure. It looks for directions with high variance, just like PCA, and high correlation with the response, making it supervised.

$$
\max_\alpha \text{Corr}^2(y, X\alpha)\text{Var}(X\alpha)
$$

# Canonical Correlation Analysis

This method finds associations between two data sets.

To find the CCA of two data matrices $X$ and $Y$ we maximise

$$
\max_{u_m, v_m} \text{Corr}^2(Yu_m, Xv_m)
$$

$$
\text{subjects to } u_mu_j = 0 \text{ and } v_mv_j=0, m\neq j, m=1,\dots,M
$$

This means that we want to maximise the correlation between a linear combination of each data set.