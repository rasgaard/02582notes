# Week 9

Exercises: https://drive.google.com/file/d/1Gxb1BN9tA9wwiXRrXrSbWwY-d1A2rvY1/view?usp=sharing
Literature: ESL Chapters: 14.3
Slides: https://drive.google.com/file/d/1lXVq1BwidW_Eldnycvb5Bvdljc8KC0hV/view?usp=sharing
Subjects: Unsupervised clustering [Hierarchical Clustering, K-means, Gaussian Mixtures, Gap statistic]

# Cluster Analysis

Unsupervised methods which groups observations together.

Given an underlying set of points, partition them into a collection of clusters so that points in the same cluster are close together while points in different clusters are far apart.

Defining the variability of points within a cluster, $c$:

$$
\text{Variability}(c) = \sum_{i\in c} \text{dist}(\text{mean}(c), x_i)^2
$$

Defining the dissimilarity of our clustering, $C$:

$$
\text{Disimilarity}(C) = \sum_{c\in C} \text{Variability(c)}
$$

With the current setup we would end up at the trivial solution of a single observation for each cluster as this would minimise the dissimilarity. We therefore impose a constraint of how many clusters we are interested in.

# Distance Metrics

## Euclidean distance

$$
d(x_i, x_j) = \sqrt{\sum_{k=1}^p (x_{ik} - x_{jk})^2}
$$

This is useful for quantitative variables. Ordinal variables can be transformed to a quantitative scale.

## Manhatten distance

$$
d(x_i, x_j) = \sum_{k=1}^p | x_{ik} - x_{jk} | 
$$

Also useful for quantitative variables. Manhatten distance is also called city block distance or Hamming distance.

## Mahalanobi distance

$$
d(x_i, x_j) = \sqrt{(x_i - x_j)^T\Sigma^{-1}(x_i - x_j)}
$$

Is related to linear discriminant analysis as we take the covariance into account. The distance is based on data itself and the two points are assumed to be of the same distribution with equal dispersion $\Sigma$. Also well suited for quantitative variables.

## Tanimoto distance

$$
d(x_i, x_j) = \frac{x_i^Tx_j}{x^T_ix_i+x^T_jx_j-x_i^Tx_j}
$$

Useful for categorical variables.

Let the sample $x$ have $x_k=1$ if it possesses the $i$th attribute and $x_k=0$ otherwise. The ratio of the number of shared attributes to the number possessed by $x_i$ or $x_j$. Often used in information retrieval and biological taxonomy.

## Final thoughts

You may have domain knowledge that some attributes are more important than others in which case you can assign weights to the attributes. The total weights must sum to 1.

# K-means Clustering

First of all we need to decide how many clusters, $K$,  there should be and randomly initialise $K$ centroids. We then follow two steps until assignments do not change:

1. Assign each point to the closest centroid.
2. Compute new centroids according to the assignments.

You may want to initialise centroids in dense areas as this is likely to reduce the number of iterations.

## K-medoids

This is an alternative method where we use one of the observations as the cluster centre. This is much more computationally heavy than K-means, but is more robust to outliers.

# Hierarchical Clustering

In this method, we do not need to define the number of clusters beforehand. 

There are two approaches

1. Bottom-up: Agglomerative (commonly used)
2. Top-down: Divisive

At each level we would perform a split or merge which gives largest between-group dissimilarity.

The first distance we will cover for hierarchical clustering is complete-linkage. It is a cluster-cluster distance measure which measures the distance from the furthest pair of points $(i, j)$ from the clusters $(G,H)$ respectively.

$$
d_\text{CL} = \max_{i\in G, j\in H} d_{ij}

$$

where the lhs is the distance between clusters and rhs is the distance between observations.

Another cluster-cluster distance measure is the single-linkage. This measures the closest pair of points $(i, j)$ from the cluster $(G, H)$ respectively

$$
d_\text{SL} = \min_{i\in G, j\in H} d_{ij}
$$

Chaining is a problem with this approach.

The last method we will go through is ward-linkage. This measures the increment in within-cluster sum of squares

$$
d_\text{Ward} = \sqrt{n_Gn_H \frac{||\bar x_G - \bar x_H||_2^2}{(n_G + n_H)}}
$$

Scaled version of the centroid-linkage distance. Wardâ€™s distance measures how much the sum of squares will increase when we merge two clusters.

$$
d_\text{Ward} = \sum_{i\in G \cup H} || x_i - \bar x_{G\cup H}||^2_2 - \sum_{i\in G}||x_i - \bar x_G||^2_2 - \sum_{i\in H} ||x_i - \bar x_H||^2_2
$$

# Clustering Validation

# Silhouette method

This method is defined as 

$$
s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}
$$

where $a(i)$ is the average distance between observation $i$ and all other observations assigned to the same cluster. $b(i)$ is the average distance between observation $i$ and all observations assigned to the neighbouring cluster (cluster where $i$ is not a member and where average distance is largest).

The average $s(i)$ over all observations is an estimate of how appropriate the points are clustered. Select number of clusters when all clusters have observations above average silhouette width, or choose $K^*$ with the maximum average silhouette.

# Within cluster dissimilarity

Distance between all points in one cluster (Euclidean distance)

$$
D_l = 2N_l \sum_{C_k(i)=l} ||x_i - \bar x_l||^2
$$

Within cluster dissimilarity

$$
W_k = \sum_l \frac{1}{2N_l}D_l
$$

# Gap-statistic

Compares the log criterion value with $K$ clusters to the expected log criterion value for uniformly distributed data, 20 simulations

$$
G(K) = \log(U_k) - \log(W_k)
$$

where $U_k$ is the within cluster dissimilarity of simulated data (mean over 20 samples) and $W_k$ is the within cluster dissimilarity of the actual data.

We would then choose 

$$
K^* = \arg\min_K\{ K | G(K) \geq (K+1) - s'_{K+1} \}
$$

where 

$$
s'_{K+1} = \text{std}(\log(U_k))\sqrt{1 + 1/20}
$$

# Gaussian Mixture Modelling

We assume that our data belongs to one of several Gaussian distributions. An unobserved (latent) random variable selects which distribution the observation comes from. This gives a complicated likelihood function but is easily solved by the expectation maximisation algorithm.

The parameters in the Gaussian mixture model are,

$$
\theta = (\pi_1, \pi_2, \pi_3, \mu_1, \mu_2, \mu_3, \Sigma_1, \Sigma_2, \Sigma_3)
$$

and $\mathbf{Z} (Z_1, \dots, Z_n)$ where $n$ might be very large.

The likelihood

$$
L(\theta; \mathbf{x}, \mathbf{Z}) = \prod_{i=1}^n \sum_{j=1}^3 1\!\!1_{\{z_i = j\}}\pi_jf(x_i; \mu_j, \Sigma_j)

$$

with maximum likelihood estimate

$$
\theta_{ML} = \arg\max_{\theta, \mathbf{Z}} \log L(\theta; \mathbf{x}, \mathbf{Z})
$$

This can be solved using the expectation maximisation algorithm:

The algorithm is a two-step iteration.

**Expectation step:** Define the expectation value,

$$
Q(\theta|\theta^k) = E_{Z|x, \theta^k}L(\theta; x, Z)
$$

**Maximisation step:** Find parameter estimate,

$$
\theta^{k+1} = \arg \max_\theta Q(\theta|\theta^k)
$$

First step defines the expectation value of the log likelihood given observed data, $x$, and current value of the parameter estimate, $\theta^k$.

Second step chooses optimal $\theta$  given the expectation value whereupon the procedure is repeated until convergence.