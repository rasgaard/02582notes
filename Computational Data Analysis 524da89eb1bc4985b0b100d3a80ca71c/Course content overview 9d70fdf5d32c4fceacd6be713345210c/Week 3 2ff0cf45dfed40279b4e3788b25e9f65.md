# Week 3

Exercises: https://drive.google.com/file/d/1pSyWLXNdEdwkUtcX8hinh0pADTCAWuxU/view?usp=sharing
Literature: ESL Chapters: 3.3, 3.4, 3.6, 18.1, 18.7
Slides: https://drive.google.com/file/d/1iEMCxpLtZqcMzvOt-yztLIJwqT1-D2RO/view?usp=sharing, https://drive.google.com/file/d/1LirVVMwGvTh2kYGbXS466o9pgSCxQq_N/view?usp=sharing
Subjects: Sparse regression and multiple testing [Lasso, Elastic Net, Bonferroni, BH, FDR]

This week’s main topic is when you have more variables than observations (when $n << p$).

# Curse of Dimensionality

What happens when our solution space, that is the number of variables, grows? The number of regions grows exponentially with the dimensionality, $D$.

## Blessing of dimensionality

1. Several features will be correlated and we can average over them.
2. Underlying distribution will be finite, informative data will lie on a low-dimensional manifold
3. Underlying structure in data (samples from continuous processes, image etc.) will give an approximate finite dimensionality.

# Subset Selection

We use subset selection in order to increase the bias as well as decrease the variance. This is in order to get the “big picture” when we are willing to sacrifice some of the small details.

There exists both forward- and backward-stepwise selection.

In **forward selection** you add variables with the highest information criterion one at a time. This results in a reasonable number of models to test and can be used when $p > n$.

In **backward elimination** we remove irrelevant features one at a time. This results in a reasonable number of models to test but has numerical issues when computing differences between models with many features. However, it is usually better than forward selection.

# Shrinkage Methods

Subset selection methods suffer from either discarding or retraining predictors, i.e. being a discrete process. This exhibits high variance and doesn’t reduce the prediction error of the full model.

Shrinkage methods are continuous and don’t suffer as much from high variability.

There are three standard techniques:

- **Ridge** regression uses quadratic shrinkage, $L_2-\text{norm}$
- **Lasso** regression uses absolute-value shrinkage, $L_1-\text{norm}$
- **Elastic net** is a hybrid of the two.

## Ridge Regression

Ridge regression solves

$$
\text{min}_\beta (Y-X\beta)^T(Y-X\beta)+\lambda\beta^T\beta
$$

or equivalently the constrained optimisation problem

$$
\text{min}_\beta (Y-X\beta)^T(Y-X\beta)~\text{subjects to }\sum\beta^2_j \leq s
$$

Increase in $\lambda$ makes the values in $\beta$ go to zero. We do not wish to penalise the intercept, $\beta_0$.

## The Lasso

The Lasso solves

$$
\text{min}_\beta (Y-X\beta)^T(Y-X\beta)+\lambda ||\beta||_1
$$

or equivalently the constrained optimisation problem, known as basis pursuit

$$
\text{min}_\beta (Y-X\beta)^T(Y-X\beta)\text{ subjects to } \sum|\beta|\leq s
$$

# Algorithms for Lasso

As the Lasso does not have a closed form solution we have to numeric algorithms to solve it. We’ll focus on two.

- Least angle regression selection (LARS)
- Cyclical coordinate descent.

## Least angle regression selection (LARS)

As it calculates the entire path (all $\lambda$ values) in the speed of one OLS fit it is very fast.

It is assumed that the data is centered and normalised (each variable has length one) such that $X^TX \approx \text{Corr}(X)$.

If a parameter estimate of an activate variable crosses zero, set it to zero and re-compute the direction.

TODO: Find Youtube explanation of LARS

## Cyclical coordinate descent

Fix $\lambda$ and solve

$$
\text{min}_\beta \frac{1}{2n} \sum^n_{i=1} (y_i - x_i\beta)^2 + \lambda | \beta | 
$$

iteratively by cyclic updating one coordinate $\beta_j$ at a time, while holding the others fixed in the current estimate $\tilde{\beta}_k$. The partial residual $r^{(j)}_i = y_i - \tilde{y}_i^{(j)}$ for $\tilde{\beta}_k$ excluding parameter $\tilde{\beta}_j$.

$$
r^{(j)}_i = y_i - \sum_{k \neq j} x_{ik} \tilde{\beta}_k ( \lambda)
$$

# Elastic net

By combining $L_1 \text{ and } L_2\text{-norm}$ we obtain sparsity and shrinkage

$$
\text{min}_\beta \frac{1}{2n} || Y-X\beta||^2_2 + \lambda \left(\frac12 (1 - \alpha)||\beta||^2_2 + \alpha ||\beta||_1 \right)
$$

Why Elastic net?

- Get rid of irrelevant variables/select important variables (Lasso)
- When $p > n$, the number of non-zero coefficients can exceed n, unlike the Lasso.
- Works well when covariates are highly correlated; allows us to “average” highly correlated features and obtain more robust estimates (grouping features).

# Multiple testing

We want to assess the significance of $p$ features. This could be done using traditional t-test of difference between groups or traditional F-test of parameter significance.

If we test one hypothesis at an $\alpha$-level of significance there is a chance $\alpha$ of falsely rejecting the hypothesis. This is no longer the case if we do many tests! **The family-wise error rate (FWER) is the probability of at least one false rejection.**

If the features are independent and each tested at an $\alpha$-level then $\text{FWER}>> \alpha$ for large $p.$

For $M$ independent test at significance level $\alpha$

$$
\text{FWER} = 1 - (1 - \alpha)^M
$$

## Bonferroni correction

To address this we can use the Bonferroni correction to rescale the $\alpha$ with the number of tests.

Reject a hypothesis if its $p$-value is below $\frac\alpha M$.

Now we have an $\alpha$-probability of making a false rejection assuming independence.

The resulting threshold will often result in **low power** and we miss out on important effects.

## False Discovery Rate (FDR)

We can have more significant findings if we allow for a few mistakes.

The false discovery rate is a technique to control the number of falsely detected significant features.

The false discovery rate is

$$
FDR = E\left(\frac{FP}{FP+TP}\right)
$$

If we accept hypothesis where $FDR < q$ then we will expect that among our findings there will be $q$ mistakes.