# Week 2

Exercises: https://drive.google.com/file/d/1JXzUxq1ZrsfEBnjuQtv-Qpid2CvZpK2u/view?usp=sharing
Literature: ESL Chapter 7. You may skip sections 7.8 and 7.9
Slides: https://drive.google.com/file/d/1oKO_Lg8qvIsLSlOrBtxBjmobgazhumJK/view?usp=sharing
Subjects: Model selection [CV, Bootstrap, Cp, AIC, BIC] Case 1 handout

# Expected Prediction Error

How we expect to perform on unseen data.

Is decomposed as 

$$
\begin{align*}
\text{Err}(x_0) =& E[(Y-\hat{f}(x_0))^2 \vert X=x_0]\\
=& \sigma_\epsilon^2 + [E\hat{f}(x_0) - f(x_0)]^2 + E[\hat{f}(x_0) - E\hat{f}(x_0)]^2 \\
=& \sigma_\epsilon^2 + \text{Bias}^2(\hat{f}(x_0)) + \text{Var}(\hat{f}(x_0)) \\
=& \text{Irreducible Error} + \text{Bias}^2 + \text{Variance}
\end{align*}
$$

# Over- and underfitting

When fitting statistical models there are parameters to tune and choices to make.

- We prefer a simple model
- We prefer models that work (Low EPE)
- These properties may contradict

We want to find the simplest model that give the best performance. If the model is too simple it may lead to underfitting while too complex may lead to overfitting.

# Bias and Variance

We overfit when the variance of the parameters estimates grow very large with the model complexity.

We would like to adhere to Occam’s razor. That is we would like to use the smallest suitable model.

To penalise large parameters we can use **regularisation**.

- $\hat{\beta} = \text{argmin}_{\beta} \vert\vert Y - X\beta\vert\vert^2_2 + \lambda\vert\vert\beta\vert\vert$
- $\vert\vert\cdot\vert\vert$ choose suitable norm

Another way to put it is the following two scenarios

- Getting it right on average but being wrong most of the time - low bias and high variance
- Never getting it quite right, but usually almost - high bias and low variance.

![Untitled](Week%202%2035c5fa63933a411da2a089d67c18a20e/Untitled.png)

## Ridge regression

This model avoids overfitting using regularisation

$\beta_{ridge} = \text{argmin}_\beta \vert\vert Y - X\beta\vert\vert^2_2 + \lambda \vert\vert\beta\vert\vert^2_2$

with the solution

$\beta_{ridge} = (X^TX + \lambda I)^{-i}(X^TY)$

Here $\lambda$ is the regularisation parameters where $\lambda = 0$ equals OLS with no bias and high variance. $\lambda \rightarrow \infty$ makes all the parameters equal to 0 resulting in high bias and no variance.

# Model Selection Tools (CV)

**Overfit** models can perform poorly on test data - high variance

**Underfit** models can perform poorly on test data - high bias

1. **Model selection:** estimating the performance of different models in order to choose the best one.
2. **Model assessment:** having chosen a final model, estimating its prediction error (generalisation error) on new data.

For both of these purposes, the best approach is to evaluate the procedure on an independent test set. If possible one should use different test data for (1) and (2). A **validation set** for (1) and a **test set** for (2).

A commonly used procedure is to split data into train, validation and test.

- The train set is used to try out different tuning parameters.
- Validation is used for calculating the EPE for different models and select the best.
- Test set is used to assess the final performance as it is completely unseen data.

A couple of points from Andrew Ng regarding choosing validation and test set:

- Val and test set should reflect data that you expect to get in the future.
- Val and test sets must come from the same distribution.
- Alternatively use a training validation set and testing validation set
- Use a single-number metric to evaluate performance.

We should make sure that the splits aren’t critical - meaning that that we should repeat the experiment with new randomised splits and report the mean and std of the error.

## Cross validation

When we don’t have sufficient data for separate validation and test sets we can use cross validation.

We split a data set into *K* equal parts randomly. For each of the parts a model is fit with parameter $\lambda$ to the other *K-1* parts, giving

$$
Err_k(\lambda) = \sum_{i\in \text{kth part}} (y_i - X_i\hat{\beta}^{\lnot k} (\lambda))^2
$$

  which results in the cross-validation error

$$
CV(\lambda) = \frac1K\sum_{k=1}^K Err_k(\lambda)
$$

Sometimes for studies with few observations and many variables we expect that CV chooses a model that is too complex. Here we use the **one standard error rule** which goes one standard error towards a simpler model. This compensates for the CV choosing a too complex model.

$$
S.E.(\lambda) = \frac{1}{\sqrt{K}} \sqrt{\frac1K \sum_{k=1}^K (Err_k(\lambda) - CV(\lambda))^2}
$$

# Considerations (Data leakage)

Observations are supposed to be **independent**. Otherwise information will **leak** from validation set to training set and we will likely overfit.

We should **normalise** inside each fold such that information don’t **leak** from outside the fold.

Missing value imputation should also happen **inside** each fold.

Long story short: **Perform all preprocessing inside folds**.

A separate test set provides a convincing, independent assessment of a models performance.

Test set results might still overestimate actual performance as a real future test set may differ in many ways from today’s data.

# Error Analysis

# Information Criteria

We know that the training error for OLS is given by 

$$
\text{Training-error} = \frac1N\sum_{i=1}^N (y_i - x_i\hat{\beta})^2
$$

and we can then imagine that for each $x_i$ we can get a new measurement $y_i^0$ that would result in the in-sample-error.

$$
\text{in-sample-error} = \frac1N\sum_{i=1}^N (y_i^0 - x_i\hat{\beta})^2
$$

Selecting the model with the smallest in-sample-error would be an effective model selection tool.

The difference between the training-error and the in-sample-error is defined as the **optimism** where 

$$
\text{expected optimism} = \frac2N\sum_{i=1}^N\text{Cov}(\hat{y}_i, y_i)
$$

The more we overfit data the greater covariance and thereby greater optimism.

A way to estimate prediction error is to estimate the optimism and then add it to the training error. The methods described - $C_p$ AIC, BIC and others - work in this way for a special class of estimates that are linear in their parameters.

### $C_p$-statistic

For the linear case with *d* variables and squared loss we have

$$
\text{expected optimism} = d\sigma^2_e
$$

If this is used in the general form of the in-sample estimates $\hat{\text{Err}}_{in} = \text{Training error} + \text{expected optimism}$ we get the $C_p$-statistic.

$$
\text{expected in-sample-error} = \text{expected training-error} + 2\frac dN \sigma^2_\epsilon
$$

Using this criterion adjust the training error that is proportional to the number of basis functions used.

### AIC - Akaike information criterion

The *A*IC is similar but more general

$$
-2 \cdot \text{E}[\log\text{Pr}_{\hat{\theta}}(Y) \sim - \frac2N \cdot\text{E}[\text{loglik}] + 2\cdot \frac dN
$$

$\text{Pr}_{\hat{\theta}}(Y)$ is a family of densities for $Y$ which also contains the “true” density. $\hat{\theta}$ is the maximum-likelihood estimate of the true parameters $\theta$ and “loglik” is the maximised log-likelihood $\text{loglik} = \sum^N_{i=1}\log \text{Pr}_{\hat{\theta}}(y_i)$.

For logistic regression using binomial log-likelihood we have

$$
\text{AIC} = -\frac2N \cdot \text{loglik} + 2 \cdot \frac2N
$$

### BIC - Bayesian information criterion

The generic form of BIC is

$$
\text{BIC} = -2\cdot \text{loglik} + (\log N) \cdot d
$$

BIC tends to penalise more complex models more heavily, giving preference to simpler models in selection. 

# Bootstrap methods

Given a training set $Z = \{z_1, z_2, \dots , z_N\}$ where $z_i=\{x_i, y_i\}$ the basic idea is to randomly draw data sets with replacement from the training data. Sampling the same size as the original training set.

This process is done $B$ times producing $B$ bootstrap data sets. Then refit the model to each of the bootstrap data sets and examine the behaviour of the fits over $B$ replications.

We can then take the mean error over the $B$ bootstrapped replications. Since we sample with replacement the bootstrap test set is not independent of the original training set. We should use the **Out-Of-Bag** samples

![Untitled](Week%202%2035c5fa63933a411da2a089d67c18a20e/Untitled%201.png)

# External resources

[https://medium.com/analytics-vidhya/model-selection-cp-aic-bic-and-adjusted-r2-6a0af25945b6](https://medium.com/analytics-vidhya/model-selection-cp-aic-bic-and-adjusted-r2-6a0af25945b6)