# Week 6

Literature: ESL Chapters: 10, 15
Slides: https://drive.google.com/file/d/1VwHB7NZSvL2zIXMRbXosuIPckiL_ryNG/view?usp=sharing
Subjects: Ensemble methods [Boosting and Random Forests]

# Random Forest

The Random Forest can be thought of as a *refinement* of bagged trees. It tries to improve on bagging by *decorrelating* the trees thus **reducing the variance**. They are simple to tune and train and are often used as a benchmark. Each tree is independent making parallelization feasible.

It works by defining a number of trees, $B$, dependent on the available computational power. We don’t really have to worry much about the number of trees as random forests **rarely overfit.**

It works by iterating $B$ times, taking a random sample of size $N$ with replacement of the data, i.e. bootstrapping. A tree is then grown until minimum node is is $n_{min}$ without pruning. For each tree, take a random sample without replacement of the predictors (of size $m<p$) and construct the first CART partition of the data. When all $B$ trees have been constructed they are returned.

The randomness in the subset of variables for each split serves as the decorrelation, lowering $\rho$ in $\rho \sigma^2 + \frac{1 - \rho}{B}\sigma^2$.

## Classification and regression

For **classification** you simple drop your data down each tree and classifies according to the majority vote of the $B$ trees.

For **regression** you also drop your data down each tree and predictions are made according to $\hat y =\frac1B \sum^B_{i=1}T_b(x)$ where $T_b(x)$ is the estimate of $x$ for each $b$th random tree.

## Model selection

To determine what parameters to use and so on we typically use the **out-of-bag (OOB) samples**, which are the samples not included in the bootstrap. These can be used for assessing each individual trees performance as we obtain their misclassification rate. Results are shown to be similar to cross validation.

There are some heuristics to pick the number of parameters. For classification `floor(sqrt(p))` and for regression `floor(p/3)`, however we should tune them a they depend on the problem, for example using OOB error estimates.

## Connection to ridge

In ridge regression we see a similar bias-variance trade-off which indicates that bagging and random forests are suitable for $n << p$ problems. The ensemble averaging in RF reduces the contribution of any one variable much like the shrinkage in ridge. This is particular when $m$, the number of candidate varables in each split, is small.

# Variable importance

For Random Forest we don’t have parameter estimates and statistical tests as we do in OLS or logistic regression. We use other measures to assess variable importance. Namely, the Gini index and an OOB estimate.

- **Gini:** The improvement in the split-criterion at each split is accumulated over all the trees for each variable.
- **OOB:** Measures prediction strength by first dropping the OOB sample down the tree, then permuting the values for the $j$th variable and computing the prediction accuracy again. An average of the difference in accuracy for all the trees gives the measure of importance for variable $j$.

 

# Boosting

Boosting is the average of many trees which are each grown to reweighted versions of the training data. Weighting *decorrelates* the tree by focusing on **regions missed** by past trees. This has the implication that you **cannot parallelize** boosting the same way as you could Random Forests as you rely on past information.

It works by growing a set of weak learners, which are just small trees, in an adaptive (meaning they use past information of which trees are the weakest in terms of misclassification, removing bias) manner to remove bias and are hence not independent. This sets it apart from Random Forest which grows large trees which causes it to have the same bias as the individual trees with averaged variance.

Weights are put on the observations and are updated to emphasise misclassifications. The final estimator is a weighted average of all the trees.

The weak learners are estimators with high bias and low variance such as small decision trees, heavily regularised linear models, K-nearest neighbours with a high number of neighbours. They only have to perform slightly better than 50% (random guessing).

## AdaBoost.M1 algorithm

1. Initialise the observation weights $w_i = 1/N, \quad i = 1,2,\dots, N.$
2. For $m = 1$ to $M$ repeat steps (a) - (d)
    1. Fit a classifier $G_m(x)$ to the training data using the weights $w_i$
    2. Compute weighted error of the newest tree $\text{err}_m = \frac{\sum_{i=1}^N}w_i I (y_i \neq G_m(x_i)){\sum_{i=1}^N w_i}$
    3. Compute $\alpha_m = \log\left[ (1 - \text{err}_m) / \text{err}_m \right]$
    4. Update weights for $i = 1, \dots , N:$
    $w_i \leftarrow w_i \exp\left[ \alpha_m I (y_i \neq G_m(x_i)) \right]$ and renormalise to $w_i$ to sum to 1
3. Output $G(x) = \text{sign}\left[ \sum_{m=1}^M \alpha_m G_m(x) \right]$

### Example

![An initial model is fitted and the misclassified samples are weighted more ](Week%206%20f64a77337ddc4de9ab9b2c042b9e5051/Untitled.png)

An initial model is fitted and the misclassified samples are weighted more 

![Another model is fitted with respect to the new weights](Week%206%20f64a77337ddc4de9ab9b2c042b9e5051/Untitled%201.png)

Another model is fitted with respect to the new weights

![This continues until some stopping point where the models are averaged.](Week%206%20f64a77337ddc4de9ab9b2c042b9e5051/Untitled%202.png)

This continues until some stopping point where the models are averaged.

Boosting was intended as a committee method like Bagging, but has the key difference that is evolves over time. The committee consists of weak learners where each cast a vote for the final prediction. Boosting dominates Bagging on most problems and therefore became the preferred choice.

Boosting is **not** known for overfitting, but it **is possible.**

Shrinkage can be performed by scaling the weights can be added by controlling the learning rate. This means that each contribution is scaled by a factor $0 < v < 1$.

## Bias and variance

Bagging and Random Forests have the same bias as the bias of an individual tree. This means that the gain obtained in prediction is due to variance reduction.

Boosting lowers bias as well as variance. Hence, we can use small trees.